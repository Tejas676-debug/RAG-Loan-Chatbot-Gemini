# -*- coding: utf-8 -*-
"""rag_loan_chatbot_gemini..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/117ABtgxch3Lt9nmeWY4c10fRo9JaRxNo
"""

!pip install -q pandas faiss-cpu sentence-transformers google-generativeai gradio

from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_csv("Training Dataset.csv")
print("Dataset Shape:", df.shape)
df.head()

documents = df.astype(str).apply(lambda row: ' | '.join(row), axis=1).tolist()

from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')  #
doc_embeddings = model.encode(documents, convert_to_tensor=False)

import faiss

dimension = doc_embeddings[0].shape[0]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(doc_embeddings))

def retrieve_top_k(query, k=20):
    query_embedding = model.encode([query])[0]
    D, I = index.search(np.array([query_embedding]), k)
    return [documents[i] for i in I[0]]

import google.generativeai as genai

genai.configure(api_key="AIzaSyDPg3h1Y6qSF0iOu4e8NSNSDaGj5l9RjV8")

def generate_response(query, context_docs):
    prompt = f"""You are a helpful assistant for loan approval analysis.

Context:
{chr(10).join(context_docs)}

Question: {query}
Answer:"""

    model = genai.GenerativeModel("gemini-1.5-flash")
    response = model.generate_content(prompt)
    return response.text

def rag_chatbot(query):
    context = retrieve_top_k(query)
    return generate_response(query, context)

print(rag_chatbot("How does credit history affect loan approval?"))

import gradio as gr

gr.Interface(fn=rag_chatbot,
             inputs="text",
             outputs="text",
             title="Loan Approval RAG Chatbot (Gemini)",
             description="Ask questions about the loan dataset!"
            ).launch()